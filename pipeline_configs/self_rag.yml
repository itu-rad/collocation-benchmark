name: "Self RAG"
listeners:
  - smi
  - dcgmi
  - top
  - iostat
pipelines:
  - name: Self RAG
    inputs: [0]
    outputs: [17]
    dataset_stage_id: 0
    loadgen:
      component: loadgen.OfflineLoadScheduler
      queue_depth: 10
      max_queries: 1
      timeout: 20000
      config:
        rate: 1 # average #requests/sec
    stages:
      - name: RAG questions dataloader
        id: 0
        outputs: [1]
        component: stages.self_rag.RAGDataLoader
        config:
          tokenizer_stage_id: 1
          batch_size: 1
      - name: Index router infer
        id: 1
        outputs: [2]
        component: stages.llm_huggingface.Inference
        config: 
          dtype: torch.bfloat16
          device: cuda
          # data_model: utils.schemas.rag.RouterAnswer
          model:
            name: microsoft/Phi-3-mini-4k-instruct
            quantize: False
            temperature: 0
      - name: Index router
        id: 2
        outputs: [3, 20]
        component: stages.self_rag.IndexRouter
        config:
          sqlite_stage_id: 3 # sql gen formatter
          llm_stage_id: 20 # llm gen formatter
      - name: SQL generator formatter
        id: 3
        outputs: [4]
        component: stages.self_rag.SQLQueryGeneratorFormatter
        polling_policy: utils.queues.polling.FirstSubmittedPolicy
        config: 
          tokenizer_stage_id: 4
          schema_stage_id: 5
      - name: SQL gen infer
        id: 4
        outputs: [5]
        component: stages.llm_huggingface.Inference
        config: 
          dtype: torch.bfloat16
          device: cuda
          # data_model: utils.schemas.rag.RouterAnswer
          model:
            name: microsoft/Phi-3-mini-4k-instruct
            quantize: False
            temperature: 0
          depends_on_id: 1
      - name: SQLite query
        id: 5
        outputs: [6]
        component: stages.self_rag.SQLiteSearch
        config:
          db_path: tmp/accounting.sqlite
      - name: Retrieval grade formatter
        id: 6
        outputs: [7]
        component: stages.self_rag.RetrievalGraderFormatter
        config: 
          tokenizer_stage_id: 7
      - name: Retrieval grader infer
        id: 7
        outputs: [8]
        component: stages.llm_huggingface.Inference
        config: 
          dtype: torch.bfloat16
          device: cuda
          # data_model: utils.schemas.rag.RouterAnswer
          model:
            name: microsoft/Phi-3-mini-4k-instruct
            quantize: False
            temperature: 0
          depends_on_id: 1
      - name: Retrieval grade router
        id: 8
        outputs: [9, 18, 17]
        component: stages.self_rag.BinaryRouter
        config: 
          max_retries: 2
          retry_is_yes: False # Query rewritter
          end_stage_id: 17 # end stage
          yes_stage_id: 9 # answer generation
          no_stage_id: 18 # query rewriter
      - name: Answer generation formatter
        id: 9
        outputs: [10]
        component: stages.self_rag.AnswerGeneratorFormatter
        polling_policy: utils.queues.polling.FirstSubmittedPolicy
        config:
          tokenizer_stage_id: 10
      - name: Answer generation infer
        id: 10
        outputs: [11]
        component: stages.llm_huggingface.Inference
        config: 
          dtype: torch.bfloat16
          device: cuda
          # data_model: utils.schemas.rag.RouterAnswer
          model:
            name: microsoft/Phi-3-mini-4k-instruct
            quantize: False
            temperature: 0
          depends_on_id: 1
      - name: Hallucination grader formatter
        id: 11
        outputs: [12]
        component: stages.self_rag.HallucinationGraderFormatter
        config: 
          tokenizer_stage_id: 12
      - name: Hallucination grader infer
        id: 12
        outputs: [13]
        component: stages.llm_huggingface.Inference
        config: 
          dtype: torch.bfloat16
          device: cuda
          # data_model: utils.schemas.rag.RouterAnswer
          model:
            name: microsoft/Phi-3-mini-4k-instruct
            quantize: False
            temperature: 0
          depends_on_id: 1
      - name: Hallucination router
        id: 13
        outputs: [14, 9, 17]
        component: stages.self_rag.BinaryRouter
        config: 
          max_retries: 2
          retry_is_yes: True # answer generation
          end_stage_id: 17 # end stage
          yes_stage_id: 9 # answer generation
          no_stage_id: 14 # answer grader
      - name: Answer grader formatter
        id: 14
        outputs: [15]
        component: stages.self_rag.AnswerGraderFormatter
        config: 
          tokenizer_stage_id: 15
      - name: Answer grader infer
        id: 15
        outputs: [16]
        component: stages.llm_huggingface.Inference
        config: 
          dtype: torch.bfloat16
          device: cuda
          # data_model: utils.schemas.rag.RouterAnswer
          model:
            name: microsoft/Phi-3-mini-4k-instruct
            quantize: False
            temperature: 0
          depends_on_id: 1
      - name: Answer grader router
        id: 16
        outputs: [17, 18]
        component: stages.self_rag.BinaryRouter
        config: 
          max_retries: 2
          retry_is_yes: False # query rewriter
          end_stage_id: 17 # end stage
          yes_stage_id: 17 # end stage
          no_stage_id: 18 # query rewrite
      - name: End stage # need to have a buffer stage in the end, because we cannot explicitly reference output queue (output stage needs to have a single output queue)
        id: 17
        component: stages.Stage
        polling_policy: utils.queues.polling.FirstSubmittedPolicy
      - name: Query rewrite formatter
        id: 18
        outputs: [19]
        component: stages.self_rag.QuestionRewriterFormatter
        config: 
          tokenizer_stage_id: 19
      - name: Query rewrite infer
        id: 19
        outputs: [3]
        component: stages.llm_huggingface.Inference
        config: 
          dtype: torch.bfloat16
          device: cuda
          # data_model: utils.schemas.rag.RouterAnswer
          model:
            name: microsoft/Phi-3-mini-4k-instruct
            quantize: False
            temperature: 0
          depends_on_id: 1
      - name: LLM answer generator formatter
        id: 20
        outputs: [21]
        component: stages.self_rag.LLMGeneratorFormatter
        polling_policy: utils.queues.polling.FirstSubmittedPolicy
        config: 
          tokenizer_stage_id: 21
      - name: LLM answer generator infer
        id: 21
        outputs: [17]
        component: stages.llm_huggingface.Inference
        config: 
          dtype: torch.bfloat16
          device: cuda
          # data_model: utils.schemas.rag.RouterAnswer
          model:
            name: microsoft/Phi-3-mini-4k-instruct
            quantize: False
            temperature: 0
          depends_on_id: 1
