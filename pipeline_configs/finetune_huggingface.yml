name: "phi3_alpaca_qlora_finetune_huggingface"
pipelines:
  - name: Alpaca finetuning
    inputs: [0]
    outputs: [1]
    dataset_stage_id: 0
    loadgen:
      component: loadgen.OfflineLoadScheduler
      queue_depth: 10
      max_queries: 2000
      timeout: 20000
      config:
        rate: 1 # average #requests/sec
    stages:
      - name: Alpaca dataloader
        id: 0
        outputs: [1]
        component: stages.llm_huggingface.HuggingFaceDataLoader
        config:
          tokenizer_stage_id: 1
          accelerator_stage_id: 1
          batch_size: 1
          shuffle: True
          split: ["train"]
          dataset:
            name: yahma/alpaca-cleaned
            system_column_name: instruction
            user_column_name: input
            assistant_column_name: output
      - name: Alpaca finetune
        id: 1
        component: stages.llm_huggingface.Finetune
        config: 
          dtype: torch.bfloat16
          device: cuda
          model:
            name: Qwen/Qwen2.5-0.5B-Instruct
            quantize: False
            lora_attn_modules: all-linear
            lora_rank: 8
            lora_alpha: 16
            lora_dropout: 0.05
          optimizer:
            component: torch.optim.AdamW
            weight_decay: 0.01
            lr: 3.0e-4
          lr_scheduler:
            name: cosine
            num_warmup_steps: 100
          gradient_accumulation_steps: 1
          accelerator: # huggingface accelerator (distributed training)
            is_enabled: True